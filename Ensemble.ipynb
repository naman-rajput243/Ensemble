{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "Ans: Ensemble Learning is a machine leaarning technique in which multiple models are combined to improve its prediction and accuracy.\n",
        "\n",
        "\n",
        "The key ideas are:  a) Instead of relying only one model it combines several models b) The prediction of the models are selected by voting, averaging or weighting c) The error is reduced as by combining the whole model the errors can be cancelled out"
      ],
      "metadata": {
        "id": "h6BokjYEYopm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Ans: Bagging uses parallel technique and it's steps are independent from each other. It uses random subset with replacement (Bootstrap). It reduces variance and improves overfitting.\n",
        "\n",
        "Boosting uses sequential technique and each steps are connected and dependent upon each other. It uses full dataset but main focus is given upon the errors made. It reduces bias and improves accuracy"
      ],
      "metadata": {
        "id": "UXznt_GuayXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Ans: The bootstrap sampling is the process of selecting random subset from the daa with replacement. This plays an important role by reducing variance by training the model with different bootstrap subsets."
      ],
      "metadata": {
        "id": "bewGS2Jcb5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Ans: OOB samples are the left out of the samples used to train the model such as in Random Forest. These are used to validate the performance of the trained model without using the test data seperately"
      ],
      "metadata": {
        "id": "-cJE1L_Nb5Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Ans: Feature importance in Decision Tree is based on how much impurity is reduced in every split.\n",
        "\n",
        "While in Random Forest it is the average of many decision trees."
      ],
      "metadata": {
        "id": "rqJn3Umab5Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "\n",
        "x = df\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"The accuracy is\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "importance = clf.feature_importances_\n",
        "name = data.feature_names\n",
        "feature_imp = pd.DataFrame({\"Feature_Name\" : name, \"Feature_Importance\" : importance})\n",
        "top_features = feature_imp.sort_values(by='Feature_Importance', ascending=False).head(5)\n",
        "\n",
        "print(top_features.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsS_bebXpHqa",
        "outputId": "01dddf9c-eaed-4006-c481-bd926a2ea9d9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 0.956140350877193\n",
            "        Feature_Name  Feature_Importance\n",
            "     worst perimeter            0.167042\n",
            "          worst area            0.139905\n",
            "worst concave points            0.136349\n",
            " mean concave points            0.079922\n",
            "           mean area            0.066801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# With multiple Decision Trees\n",
        "clf_trees = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50)\n",
        "clf_trees.fit(x_train, y_train)\n",
        "y_pred1 = clf_trees.predict(x_test)\n",
        "\n",
        "# With single Decision Tree\n",
        "clf_tree = DecisionTreeClassifier()\n",
        "clf_tree.fit(x_train, y_train)\n",
        "y_pred2 = clf_tree.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc1 = accuracy_score(y_test, y_pred1)\n",
        "acc2 = accuracy_score(y_test, y_pred2)\n",
        "\n",
        "print(\"Accuracy for multiple tree is\", acc1)\n",
        "print(\"Accuracy for single tree is\", acc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feuHcgnxpNe7",
        "outputId": "006b5487-be86-4dc0-c68c-5b4a3472ff17"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for multiple tree is 0.9666666666666667\n",
            "Accuracy for single tree is 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy \"\"\"\n",
        "\n",
        "\n",
        "import pandas\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\" : [10, 20, 50, 100],\n",
        "    \"max_depth\" : [None, 5, 10]\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid = GridSearchCV(estimator=rf, param_grid=params, cv=5)\n",
        "grid.fit(x_train, y_train)\n",
        "\n",
        "y_pred = grid.best_estimator_.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Best Parameters are\", grid.best_params_)\n",
        "print(\"Best accuracy score is\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAFpQDXQuMcy",
        "outputId": "19f5f65e-4371-434c-ab99-37e96ae7b424"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters are {'max_depth': None, 'n_estimators': 20}\n",
            "Best accuracy score is 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\"\"\"\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "data = fetch_california_housing()\n",
        "\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "bg_reg = BaggingRegressor(n_estimators=50, random_state=3)\n",
        "bg_reg.fit(x_train, y_train)\n",
        "y_pred1 = bg_reg.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=3)\n",
        "rf_reg.fit(x_train, y_train)\n",
        "y_pred2 = rf_reg.predict(x_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"Mean Squared error for Bagging Regressor is\", mean_squared_error(y_test, y_pred1))\n",
        "print(\"Mean Squared error for Random Forest Regressor is\", mean_squared_error(y_test, y_pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JurN0f4XyfPO",
        "outputId": "7c4fe2da-4a1e-4500-acc1-d876652a099f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared error for Bagging Regressor is 0.2545166646062322\n",
            "Mean Squared error for Random Forest Regressor is 0.2554810978325991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how\n",
        "\n",
        "\n",
        "1. Choosing Between Bagging and Boosting\n",
        "To decide between Bagging and Boosting:\n",
        "- Bagging (e.g., Random Forest) is preferred when the data is noisy and overfitting is a concern. It reduces variance by training multiple models on different subsets of data.\n",
        "- Boosting (e.g., XGBoost, AdaBoost) is ideal when the goal is high accuracy and the data is relatively clean. It reduces bias by sequentially correcting errors made by previous models.\n",
        "- Choice: Start with Bagging for stability and robustness. If performance plateaus, switch to Boosting for fine-tuned accuracy.\n",
        "\n",
        "2. Handling Overfitting\n",
        "Overfitting can be mitigated through:\n",
        "- Regularization: Use parameters like max_depth, min_samples_split, and learning_rate (in Boosting) to control model complexity.\n",
        "- Cross-validation: Apply k-fold cross-validation to ensure the model generalizes well.\n",
        "- Feature selection: Remove irrelevant or highly correlated features.\n",
        "- Early stopping: In Boosting, monitor validation loss and stop training when performance stops improving.\n",
        "\n",
        "3. Selecting Base Models\n",
        "Base models should be chosen based on their bias-variance characteristics:\n",
        "- Decision Trees: Common base learners for both Bagging and Boosting due to their flexibility.\n",
        "- Logistic Regression: Useful for interpretable baseline models.\n",
        "- Gradient Boosted Trees: Effective for capturing complex patterns in Boosting frameworks.\n",
        "- Justification: Decision Trees are ideal for ensemble methods because they are high-variance learners, making them suitable for Bagging, and they can be weak learners for Boosting.\n",
        "\n",
        "4. Evaluating Performance Using Cross-Validation\n",
        "To evaluate model performance:\n",
        "- Use Stratified K-Fold Cross-Validation to maintain class balance, especially important in imbalanced datasets like loan defaults.\n",
        "- Track metrics such as:\n",
        "- Accuracy: Overall correctness.\n",
        "- Precision & Recall: Especially important for default prediction.\n",
        "- F1-Score: Balances precision and recall.\n",
        "- ROC-AUC: Measures model’s ability to distinguish between classes.\n",
        "- Use GridSearchCV or RandomizedSearchCV to tune hyperparameters during cross-validation.\n",
        "\n",
        "5. Justification\n",
        "- Ensemble methods improve predictive performance by combining multiple models.\n",
        "- Bagging reduces variance and is robust to overfitting.\n",
        "- Boosting reduces bias and improves accuracy by focusing on hard-to-predict cases.\n",
        "- Cross-validation ensures the model is not just memorizing the training data but generalizing well to unseen data.\n",
        "- Proper base model selection and tuning ensures the ensemble is both powerful and efficient.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kLWkdeiq4omJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O2iocDJa44ig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}